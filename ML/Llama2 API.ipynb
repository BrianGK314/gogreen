{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kU0GYiR4hWKJ"
      },
      "source": [
        "## Install Dependencies\n",
        "- Requirements for running FastAPI Server\n",
        "- Requirements for creating a public model serving URL via Ngrok\n",
        "- Requirements for running Llama2 13B (including Quantization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDVaaatLEpzq",
        "outputId": "718c0e18-4341-4590-ec8d-727b29047c76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting llama-cpp-python\n",
            "  Downloading llama_cpp_python-0.2.11.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (1.23.5)\n",
            "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python) (5.6.3)\n",
            "Building wheels for collected packages: llama-cpp-python\n",
            "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.11-cp310-cp310-manylinux_2_35_x86_64.whl size=6423340 sha256=82683a72bf6bb69cb4096200bead1f747cca90e0870f599b9743a340078cc324\n",
            "  Stored in directory: /root/.cache/pip/wheels/dc/42/77/a3ab0d02700427ea364de5797786c0272779dce795f62c3bc2\n",
            "Successfully built llama-cpp-python\n",
            "Installing collected packages: llama-cpp-python\n",
            "Successfully installed llama-cpp-python-0.2.11\n"
          ]
        }
      ],
      "source": [
        "# Build Llama cpp\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u24fGvOmT2Pi",
        "outputId": "6a3fccb5-1a05-4eca-b555-6785c2e569e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting fastapi[all]\n",
            "  Downloading fastapi-0.104.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting uvicorn\n",
            "  Downloading uvicorn-0.23.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.5/59.5 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-multipart\n",
            "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers\n",
            "  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (1.10.13)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.13.0)\n",
            "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /usr/local/lib/python3.10/dist-packages (from fastapi[all]) (3.7.1)\n",
            "Collecting starlette<0.28.0,>=0.27.0 (from fastapi[all])\n",
            "  Downloading starlette-0.27.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# If this complains about dependency resolver, it's safe to ignore\n",
        "!pip install fastapi[all] uvicorn python-multipart transformers pydantic tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kxx3xpr2cXHS"
      },
      "outputs": [],
      "source": [
        "# This downloads and sets up the Ngrok executable in the Google Colab instance\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw3LWCfNcg9E",
        "outputId": "a3bee92b-9080-4928-b585-0bdc736ef023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.ngrok2/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# https://dashboard.ngrok.com/signup\n",
        "!./ngrok authtoken <authtoken>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUqG0_-6IChq"
      },
      "source": [
        "## Create FastAPI App\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6F078OHVssf",
        "outputId": "7846f0f2-8bac-450f-f107-337ece841c03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "from typing import Any\n",
        "\n",
        "from fastapi import FastAPI\n",
        "from fastapi import HTTPException\n",
        "from pydantic import BaseModel\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "# GGML model required to fit Llama2-13B on a T4 GPU\n",
        "GENERATIVE_AI_MODEL_REPO = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "GENERATIVE_AI_MODEL_FILE = \"llama-2-13b-chat.ggmlv3.q5_1.bin\"\n",
        "\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=GENERATIVE_AI_MODEL_REPO,\n",
        "    filename=GENERATIVE_AI_MODEL_FILE\n",
        ")\n",
        "\n",
        "llama2_model = Llama(\n",
        "    model_path=model_path,\n",
        "    n_gpu_layers=64,\n",
        "    n_ctx=2000\n",
        ")\n",
        "\n",
        "# Test an inference\n",
        "print(llama2_model(prompt=\"Hello \", max_tokens=1))\n",
        "\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "# This defines the data json format expected for the endpoint, change as needed\n",
        "class TextInput(BaseModel):\n",
        "    inputs: str\n",
        "    parameters: dict[str, Any] | None\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "def status_gpu_check() -> dict[str, str]:\n",
        "    gpu_msg = \"Available\" if tf.test.is_gpu_available() else \"Unavailable\"\n",
        "    return {\n",
        "        \"status\": \"I am ALIVE!\",\n",
        "        \"gpu\": gpu_msg\n",
        "    }\n",
        "\n",
        "\n",
        "@app.post(\"/generate/\")\n",
        "async def generate_text(data: TextInput) -> dict[str, str]:\n",
        "    try:\n",
        "        params = data.parameters or {}\n",
        "        response = llama2_model(prompt=data.inputs, **params)\n",
        "        model_out = response['choices'][0]['text']\n",
        "        return {\"generated_text\": model_out}\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PikyQUQKIewj"
      },
      "source": [
        "## Start FastAPI Server\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HErEYVtPcGg9"
      },
      "outputs": [],
      "source": [
        "# This cell finishes quickly because it just needs to start up the server\n",
        "# The server will start the model download and will take a while to start up\n",
        "# ~5 minutes\n",
        "!uvicorn app:app --host 0.0.0.0 --port 8000 > server.log 2>&1 &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJUJ4vICfQ7a",
        "outputId": "5bb5570c-ff97-4778-d53f-f773ea6cb68b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"status\":\"I am ALIVE!\",\"gpu\":\"Available\"}"
          ]
        }
      ],
      "source": [
        "# If you see \"Failed to connect\", it's because the server is still starting up\n",
        "# Wait for the model to be downloaded and the server to fully start\n",
        "# Check the server.log file to see the status\n",
        "!curl localhost:8000/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c-FpVEkifNdS"
      },
      "outputs": [],
      "source": [
        "# This starts Ngrok and creates the public URL\n",
        "from IPython import get_ipython\n",
        "get_ipython().system_raw('./ngrok http 8000 &')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## URL Endpoint below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCAIkVuxc3JU",
        "outputId": "76a1af9a-9b3b-4773-a7fb-48922360a9df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://9e67-35-203-136-52.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "# Get the Public URL\n",
        "# If this doesn't work, make sure you verified your email\n",
        "# Then run the previous code cell and this one again\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liqVEsGfZPse"
      },
      "source": [
        "## Shutting Down\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmsALxfUzRbt"
      },
      "outputs": [],
      "source": [
        "!pkill uvicorn\n",
        "!pkill ngrok"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
